{"cells":[{"cell_type":"code","source":["\"\"\"\nA CDC - Change Data Capture / Replication on Going project.\nA MySQL database will be placed in AWS RDS. \nAWS DMS will take data from RDS and write that data into temporary S3 bucket.\nNext Lambda funtion will be triggered for every new object stored in temp S3 bucket \nand invoke PySpark Job in AWS Glue.\nAll the changes to the RDS DB will be eventually replicated to the final S3 storage.\n\"\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05397c24-2319-4019-bcf1-5cb91711bebe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import when\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"CDC\").getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"957ec004-f2ec-4a7a-bd2e-55a2459b6ec9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# final load data frame\nfldf = spark.read.csv(\"/FileStore/tables/LOAD00000001.csv\")\nfldf = fldf.withColumnRenamed(\"_c0\", \"id\").withColumnRenamed(\"_c1\", \"FullName\").withColumnRenamed(\"_c2\", \"City\")\nfldf.write.mode(\"overwrite\").csv(\"/FileStore/tables/finalOutput/finalFile.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"595941c5-df35-4ca8-82f3-1cdf651a0dd1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# updated data frame\nudf = spark.read.csv(\"/FileStore/tables/20220625_164447274.csv\")\nudf = udf.withColumnRenamed(\"_c0\",\"action\").withColumnRenamed(\"_c1\", \"id\").withColumnRenamed(\"_c2\", \"FullName\").withColumnRenamed(\"_c3\", \"City\")\n# final file data frame\nffdf = spark.read.csv(\"/FileStore/tables/finalOutput/finalFile.csv\")\nffdf = ffdf.withColumnRenamed(\"_c0\", \"id\").withColumnRenamed(\"_c1\", \"FullName\").withColumnRenamed(\"_c2\", \"City\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2867d9fd-3555-406a-870e-3e1e37ecc9ef"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for row in udf.collect(): \n    if row[\"action\"] == 'U':\n        ffdf = ffdf.withColumn(\"FullName\", when(ffdf[\"id\"] == row[\"id\"], row[\"FullName\"]).otherwise(ffdf[\"FullName\"]))      \n        ffdf = ffdf.withColumn(\"City\", when(ffdf[\"id\"] == row[\"id\"], row[\"City\"]).otherwise(ffdf[\"City\"]))\n    \n    if row[\"action\"] == 'I':\n        insertedRow = [list(row)[1:]]\n        columns = ['id', 'FullName', 'City']\n        newdf = spark.createDataFrame(insertedRow, columns)\n        ffdf = ffdf.union(newdf)\n    \n    if row[\"action\"] == 'D':\n        ffdf = ffdf.filter(ffdf.id != row[\"id\"])\n        \n    \n        \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b39cb629-d2c8-496e-ada1-4bbf214e38cf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e49521d3-d43d-4fa6-9d5d-98f94c76e64f"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"CDC-Project (Glue job)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2562659289429373}},"nbformat":4,"nbformat_minor":0}
